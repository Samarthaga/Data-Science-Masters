{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "596d8fa9-1708-4ef9-adfc-0a6ef2114ca4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment 24 may Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b20b19-93d9-4e79-9023-dd02b26872e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1: An Activation Function decides whether a neuron should be activated or not. This means that it will decide whether the neuron's input to the network is important or not in the process of prediction using simpler mathematical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90abea2-bbfd-4ff5-b7b1-803c97128910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2: Sigmoid / Logistic Activation Function.\n",
    "#Tanh Function (Hyperbolic Tangent) \n",
    "#ReLU Function.\n",
    "#Leaky ReLU Function.\n",
    "#Parametric ReLU Function.\n",
    "#Exponential Linear Units (ELUs) Function.\n",
    "#Softmax Function\n",
    "#Swish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e5273-e79b-4242-99e9-184e62a456b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3: Activation functions are a critical part of the design of a neural network. The choice of activation function in the hidden layer will control how well the network model learns the training dataset. The choice of activation function in the output layer will define the type of predictions the model can make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72708fc1-3e81-499b-ab5b-bb1ca5423fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4: This function takes any real value as input and outputs values in the range of 0 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0, as shown below\n",
    "# Advantage : The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice\n",
    "# Disadvantage : It is most prone to gradient vanishing problem. Function output is not zero-centred. Power operations are relatively time-consuming which increases model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf31083-1550-4bf2-a5a1-65204757e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5 : A rectified linear unit (ReLU) is an activation function that introduces the property of non-linearity to a deep learning model and solves the vanishing gradients issue. \"It interprets the positive part of its argument. It is one of the most popular activation functions in deep learning\n",
    "# The forward and backward passes through ReLU are both just a simple \"if\" statement. Sigmoid activation, in comparison, requires computing an exponent. This advantage is huge when dealing with big networks with many neurons, and can significantly reduce both training and evaluation times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d47cf-718c-489b-aabf-9e2638aac13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6 : It doesn't allow for the activation of all of the neurons at the same time. i.e., if any input is negative, ReLU converts it to zero and doesn't allow the neuron to get activated. This means that only a few neurons are activated, making the network easy for computation\n",
    "# Efficiency: ReLu is faster to compute than the sigmoid function, and its derivative is faster to compute. This makes a significant difference to training and inference time for neural networks: only a constant factor, but constants can matter. Simplicity: ReLu is simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64741d1d-1312-4431-8873-f7f8944f2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7 : Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training\n",
    "# This is true regardless of the number of layers. LeakyReLU on the other hand, maps the values less than zeros to a very small positive number. This prevents vanishing gradient from occurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5674b6-d76f-4f8c-b728-d627bea991c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 8 : The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom.\n",
    "# The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. That is, softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaac6ad-1fdc-4ac8-b529-0c0def3839e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 9: The hyperbolic tangent activation function is also referred to simply as the Tanh (also “tanh” and “TanH“) function. It is very similar to the sigmoid activation function and even has the same S-shape. The function takes any real value as input and outputs values in the range -1 to 1\n",
    "#We observe that the gradient of tanh is four times greater than the gradient of the sigmoid function. This means that using the tanh activation function results in higher values of gradient during training and higher updates in the weights of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef6f4b-b537-4a3e-9071-a6de8e179122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c34b9c-48f0-4c6c-b612-2fffc1be707f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5e7c5-face-4a23-b92f-68c4d32e08ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba0bf3-dc2a-4445-a212-3e0e9dd50dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ec6e21-071f-4b36-be40-6d1c5207a5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34b496-5e4e-42c5-ab2f-affee23d031b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25603bf7-4223-4036-8ae7-3559b9954ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd3b18-46a8-4b6e-be11-a4826c9b62a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff3bd0-6fef-4cde-bf08-7d311558f13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a083dd-6bf2-4511-8192-458792cca540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab7910-c473-40ae-90e9-7a12d9fff767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d064c2e1-6879-4858-8f49-35f2b90dbb45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d0840-0170-431e-8cb8-af27834d6e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a08c73-13db-45c8-b12b-f1c0ccaeda60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa5cb9-5360-4ba4-b43f-5cd2b340f197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77df8ba-ddc3-473d-aba2-38708e7ba369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c9ea7-a975-4fcd-a2a4-4419e68d1b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca82ee6-fdf8-4684-827d-b880ed7102ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
