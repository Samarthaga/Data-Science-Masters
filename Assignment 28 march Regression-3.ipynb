{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7233334-11f4-4ee0-9a46-64a78d13da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 1:Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values.\n",
    "## Linear Regression establishes a relationship between dependent variable (Y) and one or more independent variables (X) using a best fit straight line (also known as regression line). Ridge Regression is a technique used when the data suffers from multicollinearity ( independent variables are highly correlated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9b01e-0f95-40bc-8b02-45bb88ab23a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 2: linearity, constant variance, and independence. However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a32934-f9a2-44ac-a8be-7c6102ffdec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 3:When λ=0, the penalty term has no effect, and ridge regression will produce the classical least square coefficients. However, as λ increases to infinite, the impact of the shrinkage penalty grows, and the ridge regression coefficients will get close zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d622f6-0847-4c52-82c6-c4d84c4237d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 4: we should avoid using ridge regression for feature selection as it shrinks the regression coefficient to zero however does not make it completely zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5660e85-9f1b-4170-8ef9-608c18ac94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 5 : Multicollinearity happens when predictor variables exhibit a correlation among themselves. Ridge regression aims at reducing the standard error by adding some bias in the estimates of the regression. The reduction of the standard error in regression estimates significantly increases the reliability of the estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1ca95e-881a-4c5d-8f15-c1eadceb7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 6 : No categorical data cannot be used for ridge regression as it seems to have some value to calculate the equation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc41b1-e94e-4661-bd94-acef43d3fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 7 : The ridge estimate is given by the point at which the ellipse and the circle touch. There is a trade-off between the penalty term and RSS. Maybe a large \\beta would give you a better residual sum of squares but then it will push the penalty term higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6948c-621f-48cb-becd-479bf7f992a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 8 : Time series regression can help you understand and predict the behavior of dynamic systems from experimental or observational data. Common uses of time series regression include modeling and forecasting of economic, financial, biological, and engineering systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
