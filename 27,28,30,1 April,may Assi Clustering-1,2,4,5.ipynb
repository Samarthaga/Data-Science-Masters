{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761b2a8e-d39f-44ef-b448-e7113661cce7",
   "metadata": {},
   "source": [
    "# Assignment 27 April Clustering-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffae4f6e-3770-4329-9eec-0f55e072446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1: K Means Algorithm,Hiararcial Clustering,DB scan Clustering,Silhoutte Scoring \n",
    "## for K means the value of k is determined to know the no of clusters and centroid is found ,\n",
    "## for Hiarchical similarity clusters are made without centroid \n",
    "## for DBSCAN handles the outliers in an amazing way \n",
    "## Silhoutte score is done in 3 steps , 1st calculate the avg. distance,then 2nd step finding the mean similarity b/w the 2 clusters and then 3rd step calculating the score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c13ba9dd-6af7-42ba-b855-58682a08001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2: K-Means clustering is an unsupervised learning algorithm. There is no labeled data for this clustering, unlike in supervised learning. K-Means performs the division of objects into clusters that share similarities and are dissimilar to the objects belonging to another cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fac7adbd-e5e2-4a41-9804-14c01c78df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3: Advantages of k-means: 1. Relatively simple to implement, 2. Scales to large data sets, 3.Guarantees convergence, 4.Can warm-start the positions of centroids\n",
    "# 5. Easily adapts to new examples, 6.Generalizes to clusters of different shapes and sizes, such as elliptical clusters\n",
    "# Limitations : k-means has trouble clustering data where clusters are of varying sizes and density. To cluster such data, you need to generalize k-means as described in the Advantages section. Clustering outliers. Centroids can be dragged by outliers, or outliers might get their own cluster instead of being ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d98e8b3d-8045-4606-b6fa-b44bd57206d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4: The silhouette coefficient may provide a more objective means to determine the optimal number of clusters. This is done by simply calculating the silhouette coefficient over a range of k, & identifying the peak as optimum K\n",
    "# Methods : The k-Means Elbow method is used to find the optimal value of the K in the K-Means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed14a01-defe-4a8e-b451-e0ad35d5ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 5: for the credit card in a bank Kmmeans Clustering can be used to find the similarity in the customer by their score of credit and salaryu and expenditure pattern \n",
    "# it helps the team to understand the optimal range of money limit that can be provdied for that cluster of group and can maintain a good and reliable data for future prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc12953-52f1-49d5-8ed9-45bd86802a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6: The output of kmeans is a list with several bits of information. The most important being: cluster : A vector of integers (from 1:k) indicating the cluster to which each point is allocated. centers : A matrix of cluster centers\n",
    "# the insights are we can identify the groups and the centroids for the datasets and can also divide further as per the requirements\n",
    "# also we can differciate the clusters via looking at it and categorization can be done easily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb300b-86d5-4526-b1d2-f25bfe438b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7: k-means has trouble clustering data where clusters are of varying sizes and density. To cluster such data, you need to generalize k-means as described in the Advantages section. Clustering outliers. Centroids can be dragged by outliers, or outliers might get their own cluster instead of being ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76957966-d99f-4c1a-b5d7-bfc890b622f3",
   "metadata": {},
   "source": [
    "# Assignment 28 April Clustering-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c01d4-62d2-4aff-ac03-09e4454e1117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1: Hierarchical clustering is a popular method for grouping objects. It creates groups so that objects within a group are similar to each other and different from objects in other groups. Clusters are visually represented in a hierarchical tree called a dendrogram\n",
    "# The difference between Kmeans and hierarchical clustering is that in Kmeans clustering, the number of clusters is pre-defined and is denoted by “K”, but in hierarchical clustering, the number of sets is either one or similar to the number of data observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626de3a2-754b-47b9-bdeb-682fab414ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2: 1: Agglomerative: Initially, each object is considered to be its own cluster. According to a particular procedure, the clusters are then merged step by step until a single cluster remains\n",
    "# 2: Divisive: The Divisive method is the opposite of the Agglomerative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cefaba-d6d8-4373-a686-7979a2adf878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3: In complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two furthest points\n",
    "# The four types of distance metrics are Euclidean Distance, Manhattan Distance, Minkowski Distance, and Hamming Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ade34b-a66e-40a4-94d9-f0859b3c283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4: To get the optimal number of clusters for hierarchical clustering, we make use a dendrogram which is tree-like chart that shows the sequences of merges or splits of clusters. If two clusters are merged, the dendrogram will join them in a graph and the height of the join will be the distance between those clusters.\n",
    "# commin methods are Kneed Locator where it tells the elbow point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af3d43-9cee-4309-b67b-440c685964e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5: A dendrogram is a tree-structured graph used in heat maps to visualize the result of a hierarchical clustering calculation. The result of a clustering is presented either as the distance or the similarity between the clustered rows or columns depending on the selected distance measure\n",
    "# Dendrogram is used to display the distance between each pair of sequentially merged objects. These are commonly used in studying hierarchical clusters before deciding the number of clusters significant to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3fded2-0100-4afa-bce4-1b516c0c91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6 : Unfortunately, the majority of clustering algorithms can only work with data that exclusively contains either numeric or categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184d40a-f5f2-4d2f-8e13-d1fad76b02cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7: n K-Means clustering outliers are found by distance based approach and cluster based approach. In case of hierarchical clustering, by using dendrogram outliers are found. The goal of the project is to detect the outlier and remove the outliers to make the clustering more reliable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5620ebd-77e5-4cf9-a1c5-e2cc93bb30f4",
   "metadata": {},
   "source": [
    "# Assignment 30 April Clustering-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef13ae-fd14-4f44-9033-01ccbab81446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1: A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class. A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb87785-e1f7-4156-90a6-95fab444b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2: V-measure cluster labeling given a ground truth. This score is identical to normalized_mutual_info_score with the 'arithmetic' option for averaging. This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won't change the score value in any way\n",
    "# This score is a measure between 0–1 that actually quantifies the goodness of the clustering partition. In fact, it requires that both homogeneity h and completeness c are maximised (NMI is 1 when both h and c are 1). Moreover if the clustering doesn't satisfy any of the two conditions NMI will be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a772e-ef94-471c-ad04-25936e938a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3 : Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually.\n",
    "# Range of silhoutte score is -1 to +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb417d1a-5a84-4a8c-ae21-29247a50d36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4 : Davies-Bouldin Index measures the size of clusters against the average distance between clusters. A lower score signifies better-defined clusters. The Davies-Bouldin Index measures the average similarity between clusters, where similarity compares the size of clusters against the between-cluster distance.\n",
    "# Illustrates the Davies Bouldin Index for different values of K ranging from K=1 to 9. Note that we can consider K=5 as the optimum number of clusters in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956baa43-e15b-4356-b229-57f6900a2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5 : Remote homologs may fail to be clustered together and instead form unnecessarily distinct clusters. The resulting clusters have high homogeneity but completeness that is too low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a48d28-218b-4d3a-9611-82adb481db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6 : This score is identical to normalized_mutual_info_score with the 'arithmetic' option for averaging.The V-measure is the harmonic mean between homogeneity and completeness\n",
    "# he V-measure is the harmonic mean between homogeneity and completeness: v = (1 + beta) * homogeneity * completeness / (beta * homogeneity + completeness) This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won't change the score value in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5968eb-0ecf-4925-bc4a-04ab8dc00252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7 : Advantages : Silhouette analysis can be used to study the separation distance between the resulting clusters and can be considered a better method compared to the Elbow method. Silhouette analysis also has added advantage to find the outliers if present in a cluster\n",
    "# Disadvatanges The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d7d28-2a8f-49ea-8c02-a911d3962368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ans 8 : The Davies-Bouldin Index (DBI) is not without its drawbacks, however. It can be sensitive to outliers and noise, leading to a false indication of poor clustering. Furthermore, it assumes a spherical shape with similar sizes and densities for each cluster, which may not be true in many real-world cases\n",
    "# reprocessing your data to remove or reduce outliers and noise, such as with normalization, standardization, or dimensionality reduction techniques, is a great place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339b527-faca-4733-bdab-e21a8bfd1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 9 : Homogeneity, completeness, and V-measure are three key related indicators of the quality of a clustering operation. In the following formulas, we will use K for the number of clusters, C for the number of classes, N for the total number of samples, and ack for the number of elements of class c in cluster k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88dae1-868e-4d77-b4af-8614d0f23128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 10: The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db896a32-de19-4b73-bbc9-918fd675ed6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 11: It is based on the idea that good clusters are those that have low within-cluster variation and high between-cluster separation. The DBI is calculated as the average of the maximum ratio of the within-cluster distance and the between-cluster distance for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86567fc1-e64b-4ff4-8983-5aacc2bb9d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ans 12 : Another metric to evaluate the quality of clustering is referred to as silhouette analysis. Silhouette analysis can be applied to other clustering algorithms as well. Silhouette coefficient ranges between −1 and 1, where a higher silhouette coefficient refers to a model with more coherent clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3643229-64b7-4a26-9cb7-8e2db5141405",
   "metadata": {},
   "source": [
    "# Assignmnet 1 May Clustering-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e93f19-6943-4f23-a25b-571bc3297afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1: A contingency table displays frequencies for combinations of two categorical variables. Analysts also refer to contingency tables as crosstabulation and two-way tables. Contingency tables classify outcomes for one variable in rows and the other in columns.\n",
    "# Accuracy, confusion matrix, log-loss, and AUC-ROC are some of the most popular metrics. Precision-recall is a widely used metrics for classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ff94a-deba-490a-af80-37af45ea8af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2: The pair confusion matrix computes a 2 by 2 similarity matrix between two clusterings by considering all pairs of samples and counting pairs that are assigned into the same or into different clusters under the true and predicted clusterings\n",
    "# A confusion matrix represents the prediction summary in matrix form. It shows how many prediction are correct and incorrect per class. It helps in understanding the classes that are being confused by model as other class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31c964-827d-4287-a399-7083af06f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3 :Extrinsic metrics measure the fairness of system outputs, which are directly related to the downstream bias that affects end users. However, they only inform the fairness of the combined system components, whereas in- trinsic metrics directly analyze the bias encoded in the contextualized language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eb5fc1-6c91-4b48-8d07-adc2c2f060d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4 : n machine learning, the performance of a classifier depends on both the classifier model and the separability/complexity of datasets. To quantitatively measure the separability of datasets, we create an intrinsic measure -- the Distance-based Separability Index (DSI), which is independent of the classifier model.\n",
    "# Difference In an intrinsic evaluation, quality of NLP systems outputs is evaluated against pre-determined ground truth (reference text) whereas an extrinsic evaluation is aimed at evaluating systems outputs based on their impact on the performance of other NLP systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7b656-26cc-4f4e-a797-75a59922917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5: A confusion matrix represents the prediction summary in matrix form. It shows how many prediction are correct and incorrect per class. It helps in understanding the classes that are being confused by model as other class\n",
    "# A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the total number of target classes. The matrix compares the actual target values with those predicted by the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a7edf-d942-4ea9-b2c1-61a6e736c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6 : When a dataset contains multiple manifolds or regions with different dimensionality (e.g., imagine sampling a line and a sphere and joining them as one dataset), they should be explored using local estimators. The idea behind local ID estimation is to operate in local neighborhoods of each point, where a manifold can be well approximated by its flat tangent space. These local pieces of data are assumed to be close to a uniformly distributed n-dimensional ball. In this package, local datasets are simply defined using the k-nearest neighbours of each point, and an ID value is thus returned for each datapoint. Such an approach also allows one to repurpose global estimators as local estimators by applying them in each local neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a956a9e-2cb4-422f-a12e-7596cd1370c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7 : Limitation 1: Imbalanced Classes. In many real-world classification problems, the classes are not evenly distributed\n",
    "# Limitation 2: Misclassification Costs\n",
    "# Limitation 3: Probability Predictions\n",
    "# Limitation 4: Ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7cbd07-5054-4956-a680-cca31c54f0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907212d-1345-439b-a649-a2d7cbc8b0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0280f856-0f1c-4a1f-b439-d8a7ce594b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ea9950-1ccf-4e33-99c0-cf082a2925e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073d25f-8334-41b2-85c0-4c8bd1a1a65d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada1422-36f9-47e6-8eae-1db225e28564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b5b61-01e0-45e3-bf66-349139111504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108f2c1-bb4b-45ee-bef6-f416cc6af07f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b4587b-575d-42f5-974c-d293583cc33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d19661d-e65b-41f2-af73-528cd17b233b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2ab9b-1172-4a8d-8976-92093016f330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3210279-f2b0-4eb0-999e-30145892602a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6032a-5dad-47ef-b872-8af43b276d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
