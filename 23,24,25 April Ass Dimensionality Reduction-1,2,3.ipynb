{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3028231-926b-472e-af9e-e3f3ef5839bc",
   "metadata": {},
   "source": [
    "# Assignment 23 April Dimensionality Reduction-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c8a47-81f7-495a-8070-7cd2c593ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1: The curse of dimensionality basically refers to the difficulties a machine learning algorithm faces when working with data in the higher dimensions, that did not exist in the lower dimensions. This happens because when you add dimensions (features), the minimum data requirements also increase rapidly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edaad25-fdba-4da7-ba7c-2f885478ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2: As the dimensionality increases, the number of data points required for good performance of any machine learning algorithm increases exponentially. The reason is that, we would need more number of data points for any given combination of features, for any machine learning model to be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2337b-09a1-409f-b10f-e6e5db05e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3: Effect of Curse of Dimensionality on Distance Functions:\n",
    "# Therefore, any machine learning algorithms which are based on the distance measure including KNN(k-Nearest Neighbor) tend to fail when the number of dimensions in the data is very high\n",
    "# it can be reduced by One of the ways to reduce the impact of high dimensions is to use a different measure of distance in a space vector. One could explore the use of cosine similarity to replace Euclidean distance . Cosine similarity can have a lesser impact on data with higher dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807ca5a7-858d-49cc-b8fe-f14785042d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 4 : Feature selection is simply selecting and excluding given features without changing them. Dimensionality reduction transforms features into a lower dimension\n",
    "# it is more of like changing the imp features and transforming into a new feature which is based on the relationship b/w the features for output to be predicted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e20b8-56e3-439c-8423-4ebb77c0adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 5: We lost some data during the dimensionality reduction process, which can impact how well future training algorithms work. It may need a lot of processing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b855f851-9733-489b-bb34-a2dc65932b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 6: KNN is very susceptible to overfitting due to the curse of dimensionality. Curse of dimensionality also describes the phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-size training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f5caa4-0fb8-4282-877a-b525a16f5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7: Eigenvalue Decomposition and Singular Value Decomposition(SVD) from linear algebra are the two main procedures used in PCA to reduce dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9cfa0-192c-4509-b68a-3ce937eab8a4",
   "metadata": {},
   "source": [
    "# Assignment 24 April Dimensionality Reduction-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41273f76-16ec-41e4-8ed2-b8b16c7b7e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1: The last step of PCA is we need to multiply Q tranpose of Q with the original data matrix in order to get the projection matrix. We go from the (d x k) Q matrix and Q transpose of Q results in d x d dimension. By multiplying the (d x n) X matrix, the projection matrix is d x n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e04fda-5c8e-4ea2-a835-4385633ba317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2: PCA can be used to reduce the dimensionality of the data by creating a set of derived variables that are linear combinations of the original variables. The values of the derived variables are given in the columns of the scores matrix Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f7c9e-bc66-43b1-afe5-b863124d6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3: Covariance-based PCA is equivalent to MLPCA whenever the variance-covariance matrix of the measurement errors is assumed diagonal with equal elements on its diagonal. The measurement error variance parameter can then be estimated by applying the probabilistic principal component analysis (PPCA) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37dbdc-5cfa-49b3-86e4-590e014cb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4: simplifies the complexity in high-dimensional data while retaining trends and patterns. It does this by transforming the data into fewer dimensions, which act as summaries of features\n",
    "# it helps us select the dimentiions to which we want our data to be transformed and is ready for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4e7d6-e0e0-4ada-8b70-10f89a498992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5: PCA, generally called data reduction technique, is very useful feature selection technique as it uses linear algebra to transform the dataset into a compressed form. We can implement PCA feature selection technique with the help of PCA class of scikit-learn Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d8a0b-c9cb-45b0-b9f9-2260ceee74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6: 1: PCA is used to visualize multidimensional data.\n",
    "        # 2 : It is used to reduce the number of dimensions in healthcare data\n",
    "        # 3: PCA can help resize an image\n",
    "        # 4 : It can be used in finance to analyze stock data and forecast returns\n",
    "        # 5 : PCA helps to find patterns in the high-dimensional datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413dc01-9f45-4a1a-98bc-907e3932ff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7: the larger the variance explained by a principal component, the more important that component is. PCA is a technique used to reduce the dimensionality of data. It does this by finding the directions of maximum variance in the data and projecting the data onto those directions\n",
    "# it is basically if the variance is high then the sperad is also very big , and if the spread is less then the variance is also comparitively low "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ed52e-a55a-47fb-9bb5-682472684ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 8: PCA works by finding the directions of maximum variance in the data set and projecting the data onto these directions. The principal components are ordered by the amount of variance they explain and are used for feature selection, data compression, clustering, and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9ef77-8990-4836-88dc-95adeafcf4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 9 : PCA works by considering the variance of each attribute because the high attribute shows the good split between the classes, and hence it reduces the dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e70c0bc-0eab-413e-86ed-7232af5d2589",
   "metadata": {},
   "source": [
    "# Assignment 25 April Dimensionality Reduction-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da8c6a-4496-4316-969a-8d25ee7a4457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1: Eigenvalues are the special set of scalar values that is associated with the set of linear equations most probably in the matrix equations. The eigenvectors are also termed as characteristic roots. It is a non-zero vector that can be changed at most by its scalar factor after the application of linear transformations.\n",
    "# numbers and vectors associated to square matrices, and together they provide the eigen-decompo- sition of a matrix which analyzes the structure of this matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9116f4d2-fd44-462e-8229-f5a91b9f0ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2: In linear algebra, eigendecomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable matrices can be factorized in this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f6f66-17ab-4927-b993-7e5d0fd23d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3:  if and only if its nilpotent part is zero. Put in another way, a matrix is diagonalizable if each block in its Jordan form has no nilpotent part; i.e., one-by-one matrix\n",
    "# with all matrices being n × n. An n × n matrix A is diagonalizable if and only if A has n linearly independent eigenvectors.\n",
    "# if the sum of the geometric multiplicities of its eigenvalues equals n which happens if and only if the geometric multiplicity of each eigenvalue is equal to its algebraic multiplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523847b-d667-468e-a318-e718ecae5775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4: The spectral theorem provides a sufficient criterion for the existence of a particular canonical form. Specifically, the spectral theorem states that if M equals the transpose of M, then M is diagonalizable: there exists an invertible matrix C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a0f7c-e8af-40bf-b727-f6a9cd1d0478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5: Eigenvalues represent magnitude, or importance. Bigger Eigenvalues correlate with more important directions. Finally, we make an assumption that more variability in a particular direction correlates with explaining the behavior of the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c72487-8a44-4651-9991-e851da179076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6: In Mathematics, an eigenvector corresponds to the real non zero eigenvalues which point in the direction stretched by the transformation whereas eigenvalue is considered as a factor by which it is stretched. In case, if the eigenvalue is negative, the direction of the transformation is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24285e2-7c8a-42e9-9dca-02a4ec904979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7 : Geometrically, an eigenvector, corresponding to a real nonzero eigenvalue, points in a direction in which it is stretched by the transformation and the eigenvalue is the factor by which it is stretched. If the eigenvalue is negative, the direction is reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f54e6a-9cdf-4f51-aebb-09d969765ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 8: It is used in car design especially car stereo system and also in decoupling three phase system. Eigendecomposition is particularly useful for analysing the structure of the data matrix in terms of the eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4d508-3004-4dcb-a29a-692770311e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 9: Since a nonzero subspace is infinite, every eigenvalue has infinitely many eigenvectors. (For example, multiplying an eigenvector by a nonzero scalar gives another eigenvector.) On the other hand, there can be at most n linearly independent eigenvectors of an n × n matrix, since R n has dimension n ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3409329-488a-4415-bf1a-617b6b50513a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ans 10 : find the eigen values and eigen vectors from the dataset and then transforms the data and results in a principle components which help in reducing the dimensionality of the data.\n",
    "# stability analysis, vibration analysis, atomic orbitals, facial recognition, and matrix diagonalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
