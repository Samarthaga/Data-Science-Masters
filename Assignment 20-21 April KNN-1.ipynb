{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2ace83-6cbc-4c81-afa4-5452bc502ec2",
   "metadata": {},
   "source": [
    "# Assignment 20 April KNN-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4a208-cadb-4a42-a51a-e9c3875f7cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Ans 1: The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. While it can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba263339-2da5-471a-85ef-f860f47bbe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2: The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7e34a-86d3-4653-a801-7f09494f13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3: The key differences are: KNN regression tries to predict the value of the output variable by using a local average. KNN classification attempts to predict the class to which the output variable belong by computing the local probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddb2a7d-7481-475c-932a-ab8de2954545",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 4: To measure the performace of the KNN we divide the data in train and test data and then perform KNN classification for binary multiclass and\n",
    "# KNN regressor for continuous output where after train test and fit we perform the predict on our test data and take accuracy_score(),confusion_matrix(),classification_report() for classification and mean_sqrt_error,mean_absolute error for regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a127dda-1aff-406e-9667-4e95112db2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5: The “Curse of Dimensionality” is a tongue in cheek way of stating that there's a ton of space in high-dimensional data sets. The size of the data space grows exponentially with the number of dimensions. This means that the size of your data set must also grow exponentially in order to keep the same density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf2bb9-8076-43b9-a8d5-4dd5fdc4e48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 6: The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space. Then we use these 'k' samples to estimate the value of the missing data points. Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5184637b-f46f-4442-95a4-a58ea7b9dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 7: KNN classifier is better for classification problems where the variable is binary or categorical , however the KNN regressor is better where the outut is an continuous value and numerical where the avg is computed for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324ab4e-15f9-4472-9a93-1c245fa43681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 8: KNN classifier It has advantages - nonparametric architecture, simple and powerful, requires no traning time, but it also has disadvantage - memory intensive, classification and estimation are slow\n",
    "# KNN can therefore be a relatively slow method compared to other regressions that may take longer to fit but then make their predictions with relatively straightforward computations. One other issue with a KNN model is that it lacks interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da81c81-2948-4159-bbaf-0b6c6d2ed6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 9: Euclidean distance is the shortest path between source and destination which is a straight line but Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always the straight lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17dbd27-ee55-41ce-95ef-e877880fc085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 10: Yes, feature scaling is required to get the better performance of the KNN algorithm. For Example, Imagine a dataset having n number of instances and N number of features. There is one feature having values ranging between 0 and 1. Meanwhile, there is also a feature that varies from -999 to 999\n",
    "# with the help of feature scaling the data representation and its range gets better via preprocessing import StandardScaler from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cfa8bd-68a7-47bc-a81b-007ec4625096",
   "metadata": {},
   "source": [
    "# Assignment 21 April KNN-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205aa99-a643-407e-8aa1-1c1cce5a755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1: # Ans 9: Euclidean distance is the shortest path between source and destination which is a straight line but Manhattan distance is sum of all the real distances between source(s) and destination(d) and each distance are always the straight lines\n",
    "## As per the performance part the euclidian distance is the direct distance to the point and is used for assumptions for traffic control while calculating the real time diff. the manhattan distance is used  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ba2a0-6737-4f0d-8733-82ca0fc439fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2: The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49aefbe-9f56-4c26-972f-73237ab98edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 3: This review attempts to answer this question through evaluating the performance (measured by accuracy, precision, and recall) of the KNN using a large number of distance measures, tested on a number of real-world data sets, with and without adding different levels of noise. The experimental results show that the performance of KNN classifier depends significantly on the distance used, and the results showed large gaps between the performances of different distances. We found that a recently proposed nonconvex distance performed the best when applied on most data sets comparing with the other tested distances. In addition, the performance of the KNN with this top performing distance degraded only ∼20% while the noise level reaches 90%, this is true for most of the distances used as well. This means that the KNN classifier using any of the top 10 distances tolerates noise to a certain degree. Moreover, the results show that some distances are less affected by the added noise comparing with other distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa9baf4-7e32-4884-87fd-08017bdf6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4: The most important hyperparameter for KNN is the number of neighbors (n_neighbors). Test values between at least 1 and 21, perhaps just the odd numbers. It may also be interesting to test different distance metrics (metric) for choosing the composition of the neighborhood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234cb6a6-c9db-4777-94b6-516daea9763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5 : So if dataset is large, there will be a lot of processing which may adversely impact the performance of the algorithm. KNN is also very sensitive to noise in the dataset. If the dataset is large, there are chances of noise in the dataset which adversely affect the performance of KNN algorithm\n",
    "## for better optimisation on large datasets Feature scaling (standardization and normalization) is required before applying the KNN algorithm to any dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e03bd-a89f-4078-a256-fc120258e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6: For large datasets, KNN can therefore be a relatively slow method compared to other regressions that may take longer to fit but then make their predictions with relatively straightforward computations. One other issue with a KNN model is that it lacks interpretability\n",
    "#Doesn’t work well with a high number of dimensions\n",
    "# Doesn’t work well with a large dataset\n",
    "# Sensitive to outliers and missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e3360-06f1-4dc8-a720-8262ec79d54a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358de85-49d2-4b9b-8f4c-6d71f100d673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc685d-2138-422c-8496-bae1d34e5e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204c1bad-f023-4dfd-aa45-8de4dc300bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a73cea-51c0-4b1d-bb22-ad379d222bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756c994-2026-4107-aa0b-644cf134aa9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb9632-40e4-48b6-8a56-5913ef48cc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f3f1eb-4069-4b40-9c23-0e32339b5d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
