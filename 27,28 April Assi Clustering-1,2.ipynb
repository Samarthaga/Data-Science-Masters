{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761b2a8e-d39f-44ef-b448-e7113661cce7",
   "metadata": {},
   "source": [
    "# Assignment 27 April Clustering-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffae4f6e-3770-4329-9eec-0f55e072446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1: K Means Algorithm,Hiararcial Clustering,DB scan Clustering,Silhoutte Scoring \n",
    "## for K means the value of k is determined to know the no of clusters and centroid is found ,\n",
    "## for Hiarchical similarity clusters are made without centroid \n",
    "## for DBSCAN handles the outliers in an amazing way \n",
    "## Silhoutte score is done in 3 steps , 1st calculate the avg. distance,then 2nd step finding the mean similarity b/w the 2 clusters and then 3rd step calculating the score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c13ba9dd-6af7-42ba-b855-58682a08001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2: K-Means clustering is an unsupervised learning algorithm. There is no labeled data for this clustering, unlike in supervised learning. K-Means performs the division of objects into clusters that share similarities and are dissimilar to the objects belonging to another cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fac7adbd-e5e2-4a41-9804-14c01c78df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3: Advantages of k-means: 1. Relatively simple to implement, 2. Scales to large data sets, 3.Guarantees convergence, 4.Can warm-start the positions of centroids\n",
    "# 5. Easily adapts to new examples, 6.Generalizes to clusters of different shapes and sizes, such as elliptical clusters\n",
    "# Limitations : k-means has trouble clustering data where clusters are of varying sizes and density. To cluster such data, you need to generalize k-means as described in the Advantages section. Clustering outliers. Centroids can be dragged by outliers, or outliers might get their own cluster instead of being ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d98e8b3d-8045-4606-b6fa-b44bd57206d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4: The silhouette coefficient may provide a more objective means to determine the optimal number of clusters. This is done by simply calculating the silhouette coefficient over a range of k, & identifying the peak as optimum K\n",
    "# Methods : The k-Means Elbow method is used to find the optimal value of the K in the K-Means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed14a01-defe-4a8e-b451-e0ad35d5ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ans 5: for the credit card in a bank Kmmeans Clustering can be used to find the similarity in the customer by their score of credit and salaryu and expenditure pattern \n",
    "# it helps the team to understand the optimal range of money limit that can be provdied for that cluster of group and can maintain a good and reliable data for future prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc12953-52f1-49d5-8ed9-45bd86802a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6: The output of kmeans is a list with several bits of information. The most important being: cluster : A vector of integers (from 1:k) indicating the cluster to which each point is allocated. centers : A matrix of cluster centers\n",
    "# the insights are we can identify the groups and the centroids for the datasets and can also divide further as per the requirements\n",
    "# also we can differciate the clusters via looking at it and categorization can be done easily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb300b-86d5-4526-b1d2-f25bfe438b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7: k-means has trouble clustering data where clusters are of varying sizes and density. To cluster such data, you need to generalize k-means as described in the Advantages section. Clustering outliers. Centroids can be dragged by outliers, or outliers might get their own cluster instead of being ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76957966-d99f-4c1a-b5d7-bfc890b622f3",
   "metadata": {},
   "source": [
    "# Assignment 28 April Clustering-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0c01d4-62d2-4aff-ac03-09e4454e1117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 1: Hierarchical clustering is a popular method for grouping objects. It creates groups so that objects within a group are similar to each other and different from objects in other groups. Clusters are visually represented in a hierarchical tree called a dendrogram\n",
    "# The difference between Kmeans and hierarchical clustering is that in Kmeans clustering, the number of clusters is pre-defined and is denoted by “K”, but in hierarchical clustering, the number of sets is either one or similar to the number of data observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626de3a2-754b-47b9-bdeb-682fab414ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 2: 1: Agglomerative: Initially, each object is considered to be its own cluster. According to a particular procedure, the clusters are then merged step by step until a single cluster remains\n",
    "# 2: Divisive: The Divisive method is the opposite of the Agglomerative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cefaba-d6d8-4373-a686-7979a2adf878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 3: In complete linkage hierarchical clustering, the distance between two clusters is defined as the longest distance between two points in each cluster. For example, the distance between clusters “r” and “s” to the left is equal to the length of the arrow between their two furthest points\n",
    "# The four types of distance metrics are Euclidean Distance, Manhattan Distance, Minkowski Distance, and Hamming Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ade34b-a66e-40a4-94d9-f0859b3c283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 4: To get the optimal number of clusters for hierarchical clustering, we make use a dendrogram which is tree-like chart that shows the sequences of merges or splits of clusters. If two clusters are merged, the dendrogram will join them in a graph and the height of the join will be the distance between those clusters.\n",
    "# commin methods are Kneed Locator where it tells the elbow point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af3d43-9cee-4309-b67b-440c685964e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 5: A dendrogram is a tree-structured graph used in heat maps to visualize the result of a hierarchical clustering calculation. The result of a clustering is presented either as the distance or the similarity between the clustered rows or columns depending on the selected distance measure\n",
    "# Dendrogram is used to display the distance between each pair of sequentially merged objects. These are commonly used in studying hierarchical clusters before deciding the number of clusters significant to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3fded2-0100-4afa-bce4-1b516c0c91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 6 : Unfortunately, the majority of clustering algorithms can only work with data that exclusively contains either numeric or categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184d40a-f5f2-4d2f-8e13-d1fad76b02cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ans 7: n K-Means clustering outliers are found by distance based approach and cluster based approach. In case of hierarchical clustering, by using dendrogram outliers are found. The goal of the project is to detect the outlier and remove the outliers to make the clustering more reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0de1d-ae3e-44d9-bced-ad5d55e58928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef13ae-fd14-4f44-9033-01ccbab81446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb87785-e1f7-4156-90a6-95fab444b95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a772e-ef94-471c-ad04-25936e938a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb417d1a-5a84-4a8c-ae21-29247a50d36f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956baa43-e15b-4356-b229-57f6900a2158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a48d28-218b-4d3a-9611-82adb481db3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5968eb-0ecf-4925-bc4a-04ab8dc00252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d7d28-2a8f-49ea-8c02-a911d3962368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
